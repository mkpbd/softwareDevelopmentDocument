


![[Screenshot_83.png]]
## Requirements clarifications

System design interview questions, by nature, are vague or abstract. Asking questions about the exact scope of the problem, and clarifying functional requirements early in the
interview is essential. Usually, requirements are divided into three parts:
1. Functional Requirements
2. Non-functional Requirements
3. Extended Requirements

## Functional requirements
These are the requirements that the end user specifically demands as basic functionalities that the system should offer. All these functionalities need to be necessarily
incorporated into the system as part of the contract.  For example:
1. "What are the features that we need to design for this system?"
2. "What are the edge cases we need to consider, if any, in our design?"


## Non-FunctionaI requirements
These are the quality constraints that the system must satisfy according to the project contract. The priority or extent to which these factors are implemented varies from
one project to another. They are also called non-behavioral requirements. For example, portability, maintainability, reliability, scalability, security, etc.
For example:
1. "Each request should be processed with the minimum latency"
2. "System should be highly available"


## Extended requirements
These are basically "nice to have" requirements that might be out of the scope of the system.
For example:
"Our system should record metrics and analytics"
"Service health and performance monitoring?" •


## Estimation and Constraints
Estimate the scale Of the system we're going to design. It is important to ask questions such as:
1. "What is the desired scale that this system will need to handle?"
2. "What is the read/write ratio of our system?"
3. "How many requests per second?
4. "How much storage will be needed?"
These questions will help us scale our design later.



## Data model design
Once we have the estimations, we can start with defining the database schema. Doing so in the early stages of the interview would help us to understand the data flow which
is the core of every system. In this step, we basically define all the entities and relationships between them.
1. "What are the different entities in the system?"
2. "What are the relationships between these entities?"
3. "How many tables do we need?"
4. "Is NoSQL a better choice here?"

## API Design
Next, we can start designing APIs for the system. These APIs will help us define the expectations from the system explicitly. We don't have to write any code, just a simple
interface defining the API requirements such as parameters, functions, classes, types, entities, etc.
For example: Create A User
It is advised to keep the interface as simple as possible and come back to it later when covering extended requirements.




## High-level component design
Now we have established our data model and API design,
it's time to identify system components (such as Load Balancers, API Gateway, etc.) that are needed to solve our
problem and draft the first design of our system.

1. "Is it best to design a monolithic or a microservices architecture?"
2. "What type of database should we use?"

Once we have a basic diagram, we can start discussing with the interviewer how the system will work from the client's perspective



## Details  component design
Now it's time to go into detail about the major components of the
system we designed. As always discuss with the interviewer which component may need further improvements.
Here is a good opportunity to demonstrate your experience in the
areas of your expertise. Present different approaches, advantages, and disadvantages. Explain your design decisions, and back them up with examples. This is also a good time to discuss any additional features the system might be able to support, though this is optional.

1. "How should we partition our data?"
2. "What about load distribution?"
3. "Should we use cache?"
4. "How will we handle a sudden spike in traffic?"


## Identify and resolve bottlenecks
Finally, it's time to discuss bottlenecks and approaches to mitigate them. Here are some important questions to ask:

1. "Do we have enough database replicas?"
2. "Is there any single point of failure?"
3. "Is database sharding required?"
4. "How can we make our system more robust?"
5. "How to improve the availability of our cache?"
Make sure to read the engineering blog of the company you're interviewing with. This will help you get a sense of what technology stack they're using and which problems are important to them.



## case Study 1
Design a scalable URL Shortener System such as TinyURL, Bitly

What is URL Shortener?
A URL shortener service creates an alias or a short URL for a
long URL. Users are redirected to the original URL when they
visit these short links.
For example, the following long URL can be changed to a
shorter URL.
Long URL:
https:(Istacklearner.com/workshop/system-design/
day-8/case-study.-l
https:/IstqckJr/efl2qt
Short URL:



Why do we need a URL
shortener?
URL shortener saves space in general when we are sharing
URLs. Users are also less likely to mistype shorter URLs.
Moreover, we can also optimize links across devices, this
allows us to track individual links.

Requirements
#### Functional requirements
Given a URL, our service should generate a shorter
and unique alias for it.
Users should be redirected to the original URL when
they visit the short link.
Links should expire after a default timespan.

#### Non-functional requirements
High availability with minimal latency.
The system should be scalable and efficient.

#### Extended requirements
Prevent abuse of services.
Record analytics and metrics for redirections.


# Estimation & Constraints

### Traffic
This will be a read-heavy system, so let's assume a 100:1
read/write ratio with 100 million links generated per month.
Writes per month: 1 x 100 million = 100 million / month
Request per second: 100 million / (30 days * 24 hours * 3600 seconds) 40 urls/second
40 write requests per second
Reads per month: 100 x 100 million = 10 billion / month
Request per second: 40*100 = 4000 requests/second


#### Bandwidth
Since we expect about 40 URLS every second, and if we
assume each request is Of size 500 bytes then the total
incoming data for write requests would be: 500 bytes KB/second
Similarly, for the read requests, since we expect about 4K
redirections, the total outgoing data would be: 4000* 500 bytes 2 MB/second


### Storage
For storage, we will assume we store each link or record in
our database for 10 years. Since we expect around 100M
new requests every month, the total number of records we
will need to store would be:100 million * 10 years x 12 months —12 Billion
Like earlier, if we assume each stored record will be
approximately 500 bytes. We will need around 6TB of
storage: 12 billion x 500 bytes = 6 TB


## Cashe
For caching, we will follow the classic Pareto principle also known as
the 80/20 rule. This means that 80% Of the requests are for 20% Of the
data, so we can cache around 20% Of our requests.
Since we get around 4K read or redirection requests each second, this
translates into 350M requests per day.
4000 urls * 24 hours * 3600 seconds —z 350 million request day
Hence, we will need around 35 GB of memory per day.
20 percent * 350 million * 500 bytes = 35 GB / day

![[Screenshot_84.png]]

## URL Encoding
Our system's primary goal is to
shorten a given URL.
But, how can we do that?
What are the challenges we will
face while creating unique short
URL?

### Base62 Approach
In this approach, we can encode the original URL using
Base62 which consists Of the capital letters A-Z, the lower
case letters a-z, and the numbers 0-9.  A-Z = 26, a-z = 26, 0-9 = 10
Total = 62   Number Of URLS = 62n
Where N: Number of characters in the generated URL.
so, if we want to generate a URL that is 7 characters long, we
will generate -3.5 trillion different URLS.
625 916 million urls
626 56.8 billion urls
62 — 3.5 trillion urls
Note: This is the simplest solution here, but it does not guarantee non-duplicate or collision-resistant keys.


## MD5 Approach
The MD5 (message-digest algorithm) is a widely used hash
function producing a 128-bit hash value (or 32 hexadecimal
32 hexadecimal digits for
generating 7 characters long URL.
MD5(original url) —+ base62 encode —+ hash
However, this creates a new issue for us, which is duplication
and collision. We can try to re-compute the hash until we find
a unique one but that will increase the overhead of our
systems. It's better to look for more scalable approaches.

## Counter Approach
In this approach, we will start with a single server which will
maintain the count of the keys generated. Once our service
receives a request, it can reach out to the counter which
returns a unique number and increments the counter. When
the next request comes the counter again returns the
unique number and this goes on.
Counter(O — 3.5 trillion) —i base62 encode hash
The problem with this approach is that it can quickly
become a single point for failure. And if we run multiple
instances Of the counter we can have collision as it's
essentially a distributed system.

## Key Generation  Service 


Key Generation Service (KGS)
AS we discussed, generating a unique key at scale without
duplication and collisions can be a bit Of a challenge. TO
solve this problem we can create a standalone Key
Generation Service (KGS) that generates a unique key
ahead of time and stores it in a separate database for later
use. This approach can make things simple for us.


## Key Generation Service (KGS)
How to handle concurrent access?
Once the key is used, we can mark it in the database to
make sure we don't reuse it, however, if there are multiple
server instances reading data concurrently, two or more
servers might try to use the same key.
The easiest way to solve this would be to store keys in two
tables. As soon as a key is used, we move it to a separate
table with appropriate locking in place. Also, to improve
reads, we can keep some of the keys in memory.
KGS database estimations
As per our discussion, we can generate up to —56.8 billion
unique 6 character long keys which will result in us having
to store 300 GB of keys.
5 chars x 916 million
6 chars x 56.8 billion
7 chars x 3.5 trillion
4.58 GB
340 GB
24500 GB or 24.5 TB
While 390 GB seems like a lot for this simple use case, it is
important to remember this is for the entirety of our service
lifetime and the size of the keys database would not
increase like our main database.


## High-level Designing 
![[Screenshot_85.png]]


## Cache Hit & Miss
Cache Hit: A cache hit describes the situation where
content is successfully served from the cache. The tags are
searched in the memory rapidly, and when the data is
found and read, it's considered a cache hit.
Cache Miss: A cache miss refers to the instance when the
memory is searched, and the data isn't found. When this
happens, the content is transferred and written into the cache.

Cache Invalidation
Cache invalidation is a process where the computer system
declares the cache entries as invalid and removes or
replaces them. If the data is modified, it should be
invalidated in the cache, if not, this can cause inconsistent
application behavior. There are three kinds Of caching
systems:
Write-through Cache
Write Around Cache
Write Back Cache

![[Screenshot_86.png]]


![[Screenshot_87.png]]

![[Screenshot_88.png]]

![[Screenshot_89.png]]


## Caching
##### Which cache eviction policy to use?
As we discussed before, we can use solutions like
Redis or Memcached and cache 20% of the daily
traffic but what kind of cache eviction policy would
best fit our needs?
Least Recently Used (LRU) can be a good policy for our
system. In this policy, we discard the least recently
used key first.
##### How to handle cache miss?
Whenever there is a cache miss, our servers can hit
the database directly and update the cache with the new entries.


### Data Partitioning
TO scale out our databases we will need to partition our
data. Horizontal partitioning (aka Sharding) can be a good
first Step. We can use partitions schemes such as:
Hash-Based Partitioning
List-Based Partitioning
Range Based Partitioning
Composite Partitioning
The above approaches can still cause uneven data and
load distribution, we can solve this using Consistent hashing.


## Database Cleanup
This is more of a maintenance step for our services and
depends on whether we keep the expired entries or remove
them. If we do decide to remove expired entries, we can
approach this in two different ways:
##### Active cleanup
In active cleanup, we will run a separate cleanup service
which will periodically remove expired links from our
storage and cache. This will be a very lightweight service
like a cron job.
##### Passive cleanup
For passive cleanup, we can remove the entry when a user
tries to access an expired link. This can ensure a lazy
cleanup of our database and cache.


## Detail Design 


	![[Screenshot_90.png]]

![[Screenshot_91.png]]
![[Screenshot_92.png]]
